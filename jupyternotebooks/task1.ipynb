{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsmOwOPDMlLy"
   },
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1\n",
    "#### Group DS_G3\n",
    "#### Members:\n",
    "*   To Minh Tuan  s4055570\n",
    "*   Huynh Huu Tri s4079860\n",
    "*   Tran Viet Duc s4106117\n",
    "*   Tran Minh Quang s4098857\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment:\n",
    "```\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "```\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook implements a comprehensive text pre-processing pipeline for customer clothing reviews, forming the foundation for subsequent analysis and classification tasks. Our approach systematically transforms raw review text into structured, normalized representations suitable for natural language processing models. The pipeline includes tokenization with sentence segmentation and word-level parsing, followed by cleaning operations including stopword removal, rare term filtering, and lemmatization to normalize word forms. We handle fashion-specific vocabulary challenges by preserving hyphenated product descriptors (e.g., \"well-made\") while removing common English stopwords.\n",
    "\n",
    "The implementation produces three distinct vocabularies—for Title text, Review Text, and their combination—with each vocabulary storing normalized tokens in alphabetical order with unique identifiers. These vocabularies support the feature representation and classification tasks in subsequent notebooks. Special attention is given to the cleaning steps that enhance signal-to-noise ratio in the text data, particularly important for clothing reviews where product descriptions contain domain-specific terminology. The resulting cleaned text and vocabulary structures provide a solid foundation for generating the vector representations needed for the machine learning classification models in Tasks 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9MTl7NaCvWA"
   },
   "source": [
    "### 1.1. Project Scope & Requirements\n",
    "\n",
    "This assignment focuses on developing a natural language processing (NLP) pipeline for analyzing and classifying clothing reviews. The project encompasses three interconnected tasks:\n",
    "\n",
    "1. **Text Pre-processing**: Implementing a comprehensive cleaning pipeline for review text data, including tokenization, stopword removal, and vocabulary construction according to specific formatting requirements.\n",
    "\n",
    "2. **Feature Representation**: Generating multiple vector representations of the review texts using both traditional bag-of-words approaches and modern word embeddings.\n",
    "\n",
    "3. **Classification Modeling**: Building and evaluating machine learning models that can predict whether a customer recommends a product based on their review text.\n",
    "\n",
    "The assignment is implemented in 2 files:\n",
    "\n",
    "1. \"task1.ipynb\": Implement the **Text Pre-processing** task.\n",
    "\n",
    "2. \"task2_3.ipynb\": Using the result of task 1 to implement the **Feature Representation** and **Classification Modeling** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptj7246eE-y8"
   },
   "source": [
    "### 1.2. Table of content\n",
    "\n",
    "The outline of this Notebook \"task1.ipynb\" is described as below:\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "    1.1 Project Scope & Requirements\n",
    "\n",
    "    1.2. Table of content\n",
    "\n",
    "    1.3. Importing libraries\n",
    "\n",
    "2. Examining and loading data\n",
    "\n",
    "3. Pre-processing data\n",
    "\n",
    "    3.1. Tokenization\n",
    "\n",
    "    3.2. Text removal and frequency calculation\n",
    "\n",
    "    3.3. Lemmatization\n",
    "\n",
    "    3.4. Fix typos\n",
    "\n",
    "    3.5. Full text preprocessing pipeline\n",
    "\n",
    "4. Saving required outputs\n",
    "\n",
    "5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoXbFFS0MlL3"
   },
   "source": [
    "### 1.3. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5932,
     "status": "ok",
     "timestamp": 1757244717438,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "u_TQEiihAkUW",
    "outputId": "13547acd-1b4c-44ba-89bd-f84e1525e8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tomin\\anaconda3\\envs\\gensim_env\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\tomin\\anaconda3\\envs\\gensim_env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\tomin\\anaconda3\\envs\\gensim_env\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\tomin\\anaconda3\\envs\\gensim_env\\lib\\site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tomin\\anaconda3\\envs\\gensim_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "# install gensim if the package is not existed -- MAY NEED TO RESTART KERNEL AFTER INSTALLATION FOR GENSIM TO WORK\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757244717453,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "qBsZ3PecMlL3",
    "outputId": "c87b0295-196d-46f2-8a97-443fed9c5e39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\tomin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to import libraries ??? FIX ME: remove all unused libs before submission\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import difflib\n",
    "import html\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramAssocMeasures, TrigramCollocationFinder, QuadgramCollocationFinder, QuadgramAssocMeasures\n",
    "from nltk.corpus import words, wordnet\n",
    "from nltk.probability import *\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics import edit_distance as levenshtein_distance\n",
    "\n",
    "# load default dataset\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Add this line to download the missing resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yS-3ANVMlL5"
   },
   "source": [
    "## 2. Examining and loading data\n",
    "+ Examine the data and explain the findings.\n",
    "+ Load the data into proper data structures and get it ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 3285002,
     "status": "aborted",
     "timestamp": 1757247302544,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "JcXHxRtMMlL5",
    "outputId": "531a9d04-1fc6-4691-cb8e-8eecc1ff7ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19662 entries, 0 to 19661\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Clothing ID              19662 non-null  int64 \n",
      " 1   Age                      19662 non-null  int64 \n",
      " 2   Title                    19662 non-null  object\n",
      " 3   Review Text              19662 non-null  object\n",
      " 4   Rating                   19662 non-null  int64 \n",
      " 5   Recommended IND          19662 non-null  int64 \n",
      " 6   Positive Feedback Count  19662 non-null  int64 \n",
      " 7   Division Name            19662 non-null  object\n",
      " 8   Department Name          19662 non-null  object\n",
      " 9   Class Name               19662 non-null  object\n",
      " 10  Clothes Title            19662 non-null  object\n",
      " 11  Clothes Description      19662 non-null  object\n",
      "dtypes: int64(5), object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Clothes Title</th>\n",
       "      <th>Clothes Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Elegant A-Line Dress</td>\n",
       "      <td>A classic A-line dress that flows gracefully, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>Petite High-Waisted Trousers</td>\n",
       "      <td>Chic, high-waisted trousers designed to elonga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>Silk Button-Up Blouse</td>\n",
       "      <td>A luxurious silk blouse with a timeless button...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Elegant A-Line Dress</td>\n",
       "      <td>A classic A-line dress that flows gracefully, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Petite Cable Knit Sweater</td>\n",
       "      <td>A cozy cable knit sweater tailored specificall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0         General         Dresses    Dresses   \n",
       "1                        0  General Petite         Bottoms      Pants   \n",
       "2                        6         General            Tops    Blouses   \n",
       "3                        4         General         Dresses    Dresses   \n",
       "4                        1  General Petite            Tops      Knits   \n",
       "\n",
       "                  Clothes Title  \\\n",
       "0          Elegant A-Line Dress   \n",
       "1  Petite High-Waisted Trousers   \n",
       "2         Silk Button-Up Blouse   \n",
       "3          Elegant A-Line Dress   \n",
       "4     Petite Cable Knit Sweater   \n",
       "\n",
       "                                 Clothes Description  \n",
       "0  A classic A-line dress that flows gracefully, ...  \n",
       "1  Chic, high-waisted trousers designed to elonga...  \n",
       "2  A luxurious silk blouse with a timeless button...  \n",
       "3  A classic A-line dress that flows gracefully, ...  \n",
       "4  A cozy cable knit sweater tailored specificall...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('assignment3_II.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAJHFp7YMlL6"
   },
   "source": [
    "## 3. Pre-processing data\n",
    "This section performs the required text pre-processing steps.\n",
    "\n",
    "In order to generate an auditable process, each single step will be implemented by a distinct functions. At the end, all the functions will be combined together by a main function to execute the whole process.\n",
    "\n",
    "To ensure the most efficient model training process later, some extra steps (such as lemmatization, typos handling) will be added into the required process to make the data as clean as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxy7fyFBMlL6"
   },
   "source": [
    "### 3.1. Tokenization\n",
    "This section generate a function to:\n",
    "+ Perform sentence segmentation.\n",
    "+ Extract tokens from *Review Text* attribute of the `df` dataset and transform into lower case format.\n",
    "+ Store result in `corpus`, of which each row store a list of tokens for a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3284482,
     "status": "aborted",
     "timestamp": 1757247302546,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "amaQlulYMlL6",
    "outputId": "6511fd45-5be8-4cdc-b69c-3f736d5c12ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "Finish tokenize df[Review Text]: 1206545 token extracted\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize pd.DataFrame to corpus (a 2D list, each row stores tokens of a review)\n",
    "def tokenize(df, attribute, get_vocab = False, print_process = False):\n",
    "  '''\n",
    "  Perform sentence segmentation and word tokenization.\n",
    "\n",
    "  Args:\n",
    "    df (pd.DataFrame): DataFrame containing text column\n",
    "    attribute (str): Name of text column\n",
    "    print_process (bool): Print the result of the tokenization process or not\n",
    "\n",
    "  Returns:\n",
    "    corpus (list of list): 2D list of tokens per row\n",
    "  '''\n",
    "  regex_tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "  ENTITY_RE = re.compile(r\"&(?:[A-Za-z]+|#[0-9]+|#x[0-9A-Fa-f]+);\")\n",
    "  corpus = []\n",
    "\n",
    "  for text in df[attribute]:\n",
    "    tokens = []\n",
    "\n",
    "    # drop HTML encode\n",
    "    unescaped = html.unescape(text)\n",
    "    soup = BeautifulSoup(unescaped, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = ENTITY_RE.sub(' ', text)\n",
    "\n",
    "    # tokenization\n",
    "    for sent in sent_tokenize(text): # sentence segmentation\n",
    "      words = regex_tokenizer.tokenize(sent) # word tokenization\n",
    "      words = [w.lower() for w in words] # lowercase transform\n",
    "      tokens.extend(words)\n",
    "    corpus.append(tokens)\n",
    "\n",
    "  # Build vocab dict (alphabetical order)\n",
    "  if get_vocab:\n",
    "    unique_tokens = sorted({t for doc in corpus for t in doc})\n",
    "    vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "    return corpus, vocab\n",
    "\n",
    "  # Print process\n",
    "  if print_process:\n",
    "    print(f\"Finish tokenize df[{attribute}]: {sum([len(tokens) for tokens in corpus])} token extracted\")\n",
    "\n",
    "  return corpus\n",
    "\n",
    "# demo use of tokenize() function\n",
    "print('*** This is just demo only ***')\n",
    "corpus = tokenize(df, 'Review Text', print_process = True) # each row in corpus store all tokens of a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS0CHucRjiH4"
   },
   "source": [
    "### 3.2. Detect collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3274719,
     "status": "aborted",
     "timestamp": 1757247302548,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "cQKF4kyQbeQs",
    "outputId": "f8b836f6-749b-4396-eaa8-173322583c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "\n",
      "=== Top 20 Quadgram Collocations ===\n",
      "1. 'had such high hopes'\n",
      "2. 'those of us who'\n",
      "3. 'you raise your arms'\n",
      "4. 'as another reviewer noted'\n",
      "5. 'a few weeks ago'\n",
      "6. 'angel of the north'\n",
      "7. 'after reading other reviews'\n",
      "8. 'was pleasantly surprised by'\n",
      "9. 'if you're between sizes'\n",
      "10. 'as another reviewer mentioned'\n",
      "11. 'have paid full price'\n",
      "12. 'received tons of compliments'\n",
      "13. 'inches above my knee'\n",
      "14. 'paid full price for'\n",
      "15. 'say enough good things'\n",
      "16. 'get compliments every time'\n",
      "17. 'deal breaker for me'\n",
      "18. 'other reviewers have noted'\n",
      "19. 'love at first sight'\n",
      "20. 'i've received many compliments'\n",
      "\n",
      "=== Top 20 Trigram Collocations ===\n",
      "1. 'wide rib cage'\n",
      "2. 'worth every penny'\n",
      "3. 'few weeks ago'\n",
      "4. 'start by saying'\n",
      "5. 'hd in paris'\n",
      "6. 'exceeded my expectations'\n",
      "7. 'i've ever owned'\n",
      "8. 'with flip flops'\n",
      "9. 'dry clean only'\n",
      "10. 'pleasantly surprised by'\n",
      "11. 'paid full price'\n",
      "12. 'received numerous compliments'\n",
      "13. 'paying full price'\n",
      "14. 'pay full price'\n",
      "15. 'hard time finding'\n",
      "16. 'stop thinking about'\n",
      "17. 'another reviewer noted'\n",
      "18. 'an added bonus'\n",
      "19. 'raise your arms'\n",
      "20. 'my rib cage'\n",
      "\n",
      "=== Top 20 Bigram Collocations ===\n",
      "1. 'meadow rue'\n",
      "2. 'byron lars'\n",
      "3. 'tracy reese'\n",
      "4. 'customer service'\n",
      "5. 'flip flops'\n",
      "6. 'bridal shower'\n",
      "7. 'fingers crossed'\n",
      "8. 'rib cage'\n",
      "9. 'personal preference'\n",
      "10. 'southern california'\n",
      "11. 'hour glass'\n",
      "12. 'holding horses'\n",
      "13. 'hei hei'\n",
      "14. 'polka dot'\n",
      "15. 'thank goodness'\n",
      "16. 'sales associate'\n",
      "17. 'stumbled upon'\n",
      "18. 'potato sack'\n",
      "19. 'gentle cycle'\n",
      "20. 'visual interest'\n",
      "\n",
      "Note: Your min_freq=10 may be too low for a corpus with 1206545 tokens.\n",
      "Consider using min_freq≥120 for more meaningful collocations.\n",
      "['had-such-high-hopes', 'those-of-us-who', 'you-raise-your-arms', 'as-another-reviewer-noted', 'a-few-weeks-ago', 'angel-of-the-north', 'after-reading-other-reviews', 'was-pleasantly-surprised-by', \"if-you're-between-sizes\", 'as-another-reviewer-mentioned', 'have-paid-full-price', 'received-tons-of-compliments', 'inches-above-my-knee', 'paid-full-price-for', 'say-enough-good-things', 'get-compliments-every-time', 'deal-breaker-for-me', 'other-reviewers-have-noted', 'love-at-first-sight', \"i've-received-many-compliments\", 'wide-rib-cage', 'worth-every-penny', 'few-weeks-ago', 'start-by-saying', 'hd-in-paris', 'exceeded-my-expectations', \"i've-ever-owned\", 'with-flip-flops', 'dry-clean-only', 'pleasantly-surprised-by', 'paid-full-price', 'received-numerous-compliments', 'paying-full-price', 'pay-full-price', 'hard-time-finding', 'stop-thinking-about', 'another-reviewer-noted', 'an-added-bonus', 'raise-your-arms', 'my-rib-cage', 'meadow-rue', 'byron-lars', 'tracy-reese', 'customer-service', 'flip-flops', 'bridal-shower', 'fingers-crossed', 'rib-cage', 'personal-preference', 'southern-california', 'hour-glass', 'holding-horses', 'hei-hei', 'polka-dot', 'thank-goodness', 'sales-associate', 'stumbled-upon', 'potato-sack', 'gentle-cycle', 'visual-interest']\n"
     ]
    }
   ],
   "source": [
    "def find_collocations(corpus, ngram_range = [2, 3, 4], num_collocations = 20, min_freq = 10, print_process = False):\n",
    "    '''\n",
    "    Find n-gram collocations (bigrams, trigrams, fourgrams) in the corpus using PMI measure\n",
    "\n",
    "    Args:\n",
    "        corpus (list of list): 2D token list\n",
    "        num_collocations (int): Number of top collocations to return per n-gram type\n",
    "        min_freq (int): Minimum frequency for collocations\n",
    "        print_process (bool): Whether to print results\n",
    "        ngram_range (tuple): Range of n-grams to find (min_n, max_n)\n",
    "\n",
    "    Returns:\n",
    "        collocations_dict (dict): Dictionary of top collocations by n-gram type\n",
    "    '''\n",
    "    # Import necessary modules\n",
    "\n",
    "    # Flatten corpus for collocation finding\n",
    "    all_tokens = [token for doc in corpus for token in doc]\n",
    "\n",
    "    # Dictionary to store collocations by n-gram size\n",
    "    collocations = []\n",
    "\n",
    "    # Find quadgrams if in range\n",
    "    if 4 in ngram_range:\n",
    "        finder = QuadgramCollocationFinder.from_words(all_tokens)\n",
    "        finder.apply_freq_filter(min_freq)\n",
    "        quadgram_measures = QuadgramAssocMeasures()\n",
    "        top_quadgrams = finder.nbest(quadgram_measures.pmi, num_collocations)\n",
    "        collocations.extend([f\"{w1}-{w2}-{w3}-{w4}\" for w1, w2, w3, w4 in top_quadgrams])\n",
    "\n",
    "        if print_process:\n",
    "            print(f\"\\n=== Top {num_collocations} Quadgram Collocations ===\")\n",
    "            for i, collocation in enumerate(top_quadgrams, 1):\n",
    "                print(f\"{i}. '{' '.join(collocation)}'\")\n",
    "\n",
    "    # Find trigrams if in range\n",
    "    if 3 in ngram_range:\n",
    "        finder = TrigramCollocationFinder.from_words(all_tokens)\n",
    "        finder.apply_freq_filter(min_freq)\n",
    "        trigram_measures = TrigramAssocMeasures()\n",
    "        top_trigrams = finder.nbest(trigram_measures.pmi, num_collocations)\n",
    "        collocations.extend([f\"{w1}-{w2}-{w3}\" for w1, w2, w3 in top_trigrams])\n",
    "\n",
    "        if print_process:\n",
    "            print(f\"\\n=== Top {num_collocations} Trigram Collocations ===\")\n",
    "            for i, collocation in enumerate(top_trigrams, 1):\n",
    "                print(f\"{i}. '{' '.join(collocation)}'\")\n",
    "\n",
    "    # Find bigrams if in range\n",
    "    if 2 in ngram_range:\n",
    "        finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "        finder.apply_freq_filter(min_freq)\n",
    "        bigram_measures = BigramAssocMeasures()\n",
    "        top_bigrams = finder.nbest(bigram_measures.pmi, num_collocations)\n",
    "        collocations.extend([f\"{w1}-{w2}\" for w1, w2 in top_bigrams])\n",
    "\n",
    "        if print_process:\n",
    "            print(f\"\\n=== Top {num_collocations} Bigram Collocations ===\")\n",
    "            for i, collocation in enumerate(top_bigrams, 1):\n",
    "                print(f\"{i}. '{' '.join(collocation)}'\")\n",
    "\n",
    "    # Calculate optimal min_freq based on corpus size\n",
    "    if print_process:\n",
    "        total_tokens = len(all_tokens)\n",
    "        recommended_min_freq = max(3, int(total_tokens * 0.0001))\n",
    "        if min_freq < recommended_min_freq:\n",
    "            print(f\"\\nNote: Your min_freq={min_freq} may be too low for a corpus with {total_tokens} tokens.\")\n",
    "            print(f\"Consider using min_freq≥{recommended_min_freq} for more meaningful collocations.\")\n",
    "\n",
    "    return collocations\n",
    "\n",
    "# demo use of find_collocations() function\n",
    "print('*** This is just demo only ***')\n",
    "corpus = tokenize(df, 'Review Text')\n",
    "collocations = find_collocations(corpus, print_process = True, ngram_range = [2, 3, 4])  # Find bigrams through quadgrams\n",
    "print(collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3247083,
     "status": "aborted",
     "timestamp": 1757247302549,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "_A32C63wPI7g",
    "outputId": "c8992f6d-fda4-4e61-f2bc-620b71863aaa"
   },
   "outputs": [],
   "source": [
    "def add_collocations(corpus, collocations, print_process = False):\n",
    "  '''\n",
    "  Add collocations to the corpus\n",
    "\n",
    "  Args:\n",
    "      corpus (list of list): 2D token list\n",
    "      collocations_dict (dict): Dictionary of top collocations by n-gram type\n",
    "\n",
    "  Returns:\n",
    "      corpus (list of list): Corpus with detected collocations treated as single tokens (e.g., 'new-york')\n",
    "      replaced_tokens (dict): All tokens that have been replaced by collocations in a form {token_be_replaced: collocation}\n",
    "  '''\n",
    "  result_corpus = []\n",
    "\n",
    "  for doc in corpus:\n",
    "    doc = ' '.join(doc) # doc is transfromed from ['he', 'work', 'out', ...] to 'he work out ...' so all collocations will be separated with space\n",
    "    for collocation in collocations:\n",
    "      collocation_with_space = collocation.replace('-', ' ')\n",
    "      doc = doc.replace(collocation_with_space, collocation) # replace all collocation with space to collocation with \"-\", like 'work out' to 'work-out'\n",
    "    doc = doc.split(' ') # doc is transformed to ['he', 'work-out', ...]\n",
    "    result_corpus.append(doc)\n",
    "\n",
    "  if print_process:\n",
    "    print(f\"Finish add collocations:\")\n",
    "    print(f\"+ Before: {sum([len(review_tokens) for review_tokens in corpus])} tokens: {corpus[:5]} ...\")\n",
    "    print(f\"+ Now:    {sum([len(review_tokens) for review_tokens in result_corpus])} tokens: {result_corpus[:5]} ...\")\n",
    "\n",
    "  return result_corpus\n",
    "\n",
    "# demo use of add_collocations() function\n",
    "print('*** This is just demo only ***')\n",
    "corpus = tokenize(df, 'Review Text')\n",
    "collocations = find_collocations(corpus, ngram_range = [2, 3, 4])  # Find bigrams through quadgrams\n",
    "new_corpus = add_collocations(corpus, collocations, print_process = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69t7twd0MlL7"
   },
   "source": [
    "### 3.2. Text removal and frequency calculation\n",
    "This section generates 2 functions:\n",
    "+ A function `calc_frequency()` to calculate term frequency and document frequency of `corpus` (2D list, each row store tokens of a review).\n",
    "+ A function `remove_tokens()` to remove tokens from `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3220029,
     "status": "aborted",
     "timestamp": 1757247302551,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "LVRvb13BMlL8",
    "outputId": "97ff0de2-88d5-4955-b65f-d05f3d07acd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "Finish calculate term frequency of 1206545 tokens\n",
      "Finish calculate document frequency of 1206545 tokens\n",
      "*** Term frequency ***\n",
      " <FreqDist with 14806 samples and 1206545 outcomes>\n",
      "*** Document frequency ***\n",
      " <FreqDist with 14806 samples and 898450 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate frequency\n",
    "def calc_frequency(corpus, type = 'term', print_process = False):\n",
    "  '''\n",
    "  Calculate token frequency. Term frequency (type = \"term\") or document frequency (type = \"document\")\n",
    "\n",
    "  Args:\n",
    "    corpus (list of list): 2D token list\n",
    "    type (str): \"term\" or \"document\"\n",
    "    print_process (bool): Print the result of the tokenization process or not\n",
    "\n",
    "  Returns:\n",
    "    frequency (dict): token -> frequency\n",
    "  '''\n",
    "  if type == 'term': # if type is 'term'\n",
    "    all_tokens = [token for doc in corpus for token in doc] # extract all tokens of all documents into 1 list\n",
    "    frequency = FreqDist(all_tokens)\n",
    "  else:\n",
    "    all_unique_tokens_in_each_doc = [token for doc in corpus for token in set(doc)] # filter unique tokens in each document (set(doc)), then extract them into 1 list\n",
    "    frequency = FreqDist(all_unique_tokens_in_each_doc)\n",
    "\n",
    "  # Print process\n",
    "  if print_process:\n",
    "    print(f\"Finish calculate {type} frequency of {sum([len(tokens) for tokens in corpus])} tokens\")\n",
    "\n",
    "  return frequency\n",
    "\n",
    "# demo use of calc_frequency() function\n",
    "print('*** This is just demo only ***')\n",
    "term_frequency = calc_frequency(corpus, type = 'term', print_process = True)\n",
    "document_frequency = calc_frequency(corpus, type = 'document', print_process = True)\n",
    "print('*** Term frequency ***\\n', term_frequency)\n",
    "print('*** Document frequency ***\\n', document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3219191,
     "status": "aborted",
     "timestamp": 1757247302553,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "E2rgsMvzMlL8",
    "outputId": "4b3e991c-54d1-465d-bed8-7e5731c1d21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "Finish removal:\n",
      "+ Before: 1206545 tokens: [['i', 'had', 'such', 'high', 'hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['i', 'love', 'tracy', 'reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "+ Now:    1109548 tokens: [['had', 'such', 'high', 'hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'could', 'not', 'zip', 'it', 'up', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it'], ['love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'wear', 'it', 'get', 'nothing', 'but', 'great', 'compliments'], ['this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['love', 'tracy', 'reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'returned', 'this', 'dress'], ['aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'waas', 'trying', 'on', 'of', 'which', 'kept', 'all', 'oops']] ...\n"
     ]
    }
   ],
   "source": [
    "# Function to remove invalid tokens\n",
    "def remove_tokens(corpus, tokens_to_remove, remove_single_char = False, print_process = False):\n",
    "  '''\n",
    "  Remove the tokens of `corpus` that are in `tokens_to_remove`\n",
    "\n",
    "  Args:\n",
    "    corpus (list of list): tokenized text\n",
    "    tokens_to_remove (list): list of tokens (str)\n",
    "    remove_single_char (bool): whether removing tokens with length = 1 or not\n",
    "\n",
    "  Returns:\n",
    "    corpus (list of list): cleaned tokenized text\n",
    "  '''\n",
    "\n",
    "  tokens_to_remove = set(tokens_to_remove)\n",
    "  cleaned_corpus = []\n",
    "  for doc in corpus:\n",
    "    cleaned_doc = [w for w in doc if (w not in tokens_to_remove) and ((not remove_single_char) or len(w) >= 2)]\n",
    "    cleaned_corpus.append(cleaned_doc)\n",
    "\n",
    "  # Print process\n",
    "  if print_process:\n",
    "    print(\"Finish removal:\")\n",
    "    print(f\"+ Before: {sum([len(review_tokens) for review_tokens in corpus])} tokens: {corpus[:5]} ...\")\n",
    "    print(f\"+ Now:    {sum([len(review_tokens) for review_tokens in cleaned_corpus])} tokens: {cleaned_corpus[:5]} ...\")\n",
    "\n",
    "  return cleaned_corpus\n",
    "\n",
    "# demo use of remove_tokens() function\n",
    "print('*** This is just demo only ***')\n",
    "with open(\"stopwords_en.txt\", \"r\", encoding=\"utf-8\") as file: # Download stopwords_en.txt\n",
    "  stop_words = set(word for word in file)\n",
    "  corpus_cleaned = remove_tokens(corpus, stop_words, remove_single_char = True, print_process = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzuK_uzBMlL9"
   },
   "source": [
    "### 3.3. Lemmatization\n",
    "This section generates a function to:\n",
    "+ Lemmatize each token in `corpus` (2D list, each row stores tokens of a review in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3219079,
     "status": "aborted",
     "timestamp": 1757247302567,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "Ff2kRIeiMlL-",
    "outputId": "8fce6e9c-e480-4c8e-8421-ab22afdfeb71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "Finish lemmatize:\n",
      "+ Before: 1206545 tokens: [['i', 'had', 'such', 'high', 'hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['i', 'love', 'tracy', 'reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "+ Now:    1206545 tokens: [['i', 'have', 'such', 'high', 'hope', 'for', 'this', 'dress', 'and', 'really', 'want', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'order', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'find', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reorder', 'it', 'in', 'petite', 'medium', 'which', 'be', 'just', 'ok', 'overall', 'the', 'top', 'half', 'be', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'have', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layer', 'imo', 'a', 'major', 'design', 'flaw', 'be', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliment'], ['this', 'shirt', 'be', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'be', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'legging', 'and', 'it', 'be', 'sleeveless', 'so', 'it', 'pair', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['i', 'love', 'tracy', 'reese', 'dress', 'but', 'this', 'one', 'be', 'not', 'for', 'the', 'very', 'petite', 'i', 'be', 'just', 'under', 'foot', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'be', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'it', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'be', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelm', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alteration', 'shorten', 'and', 'narrow', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'do', 'not', 'work', 'on', 'me', 'i', 'return', 'this', 'dress'], ['i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'go', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'be', 'so', 'pale', 'hte', 'color', 'be', 'really', 'gorgeous', 'and', 'turn', 'out', 'it', 'mathced', 'everythiing', 'i', 'be', 'try', 'on', 'with', 'it', 'prefectly', 'it', 'be', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'x', 'be', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decide', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'say', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pant', 'and', 'the', 'skirt', 'i', 'waas', 'try', 'on', 'of', 'which', 'i', 'keep', 'all', 'oops']] ...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "def lemmatize(corpus, print_process = False):\n",
    "  '''\n",
    "  Apply lemmatization to 2D token list.\n",
    "\n",
    "  Args:\n",
    "    corpus (list of list)\n",
    "\n",
    "  Returns:\n",
    "    corpus (list of list): lemmatized tokens\n",
    "  '''\n",
    "  result_corpus = []\n",
    "  pos_map = {\n",
    "    'ADJ': 'a',\n",
    "    'ADP': 's',\n",
    "    'ADV': 'r',\n",
    "    'NOUN': 'n', # assume any undefined tags (like DET, PRON, ...) is n (NOUN)\n",
    "    'VERB': 'v',\n",
    "  }\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for doc in corpus:\n",
    "    doc_with_tag = nltk.pos_tag(doc, tagset = 'universal') # set POS tag for all tokens in doc (tag is the type of word: NOUN, ADJ, ...)\n",
    "    lemmatized_doc = [lemmatizer.lemmatize(token, pos_map.get(tag, 'n')) for token, tag in doc_with_tag] # assume any undefined tags (like DET, PRON, ...) is n (NOUN)\n",
    "    result_corpus.append(lemmatized_doc)\n",
    "\n",
    "  # Print process\n",
    "  if print_process:\n",
    "    print(\"Finish lemmatize:\")\n",
    "    print(f\"+ Before: {sum([len(review_tokens) for review_tokens in corpus])} tokens: {corpus[:5]} ...\")\n",
    "    print(f\"+ Now:    {sum([len(review_tokens) for review_tokens in result_corpus])} tokens: {result_corpus[:5]} ...\")\n",
    "  return result_corpus\n",
    "\n",
    "# demo use of lemmatize() function\n",
    "print('*** This is just demo only ***')\n",
    "corpus = tokenize(df, 'Review Text')\n",
    "corpus_cleaned = lemmatize(corpus, print_process = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVZm93P_cI-b"
   },
   "source": [
    "### 3.4. Fix typos\n",
    "This section generates a function to fix all potential typos in `corpus` (2D list, each row stores tokens of a review in the dataset). The method used is the combination of frequent-based strategy and built-in dictionary look up:\n",
    "1. Extract vocabulary of `corpus` with term frequencies.\n",
    "2. If a token is in built-in dictionary (from NLTK library), it is a correct word. Otherwise, it is a potential typos (still need to check further since it may be a correct special terminology).\n",
    "3. If a token is similar to a word that is supposed to be correct, and the token appears less frequently than that word, the token is supposed to be a typo (frequent-based approach).\n",
    "4. If a token is not similar to any other correct words which appear more frequent than it, that token may be correct (frequent-based approach).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3146613,
     "status": "aborted",
     "timestamp": 1757247302569,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "EV_5_r0ibbL4",
    "outputId": "e831e79f-9a5c-4818-8b4d-2841124331ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "Fix: buttondowns -> buttondown (similar score: 0.9523809523809523)\n",
      "Fix: comnfortable -> confortable (similar score: 0.9565217391304348)\n",
      "Fix: descriptions -> description (similar score: 0.9565217391304348)\n",
      "Fix: differences -> difference (similar score: 0.9523809523809523)\n",
      "Fix: disapppointing -> disappointing (similar score: 0.9629629629629629)\n",
      "Fix: dissappointed -> dissapointed (similar score: 0.96)\n",
      "Fix: embellishments -> embellishment (similar score: 0.9629629629629629)\n",
      "Fix: emroidered -> embroidered (similar score: 0.9523809523809523)\n",
      "Fix: eyecatching -> eye-catching (similar score: 0.9565217391304348)\n",
      "Fix: pear-shape -> pear-shaped (similar score: 0.9523809523809523)\n",
      "Fix: shortwaisted -> short-waisted (similar score: 0.96)\n",
      "Fix: silhouettes -> silhouette (similar score: 0.9523809523809523)\n",
      "Fix: suprisingly -> surprisingly (similar score: 0.9565217391304348)\n",
      "Fix: sweatshirts -> sweatshirt (similar score: 0.9523809523809523)\n",
      "Fix: turtlenceck -> turtleneck (similar score: 0.9523809523809523)\n",
      "Finish fix typos:\n",
      "+ Before: 65368 tokens: [['some', 'major', 'design', 'flaws'], ['my', 'favorite', 'buy'], ['flattering', 'shirt'], ['not', 'for', 'the', 'very', 'petite'], ['cagrcoal', 'shimmer', 'fun']] ...\n",
      "+ Now:    65368 tokens: [['some', 'major', 'design', 'flaws'], ['my', 'favorite', 'buy'], ['flattering', 'shirt'], ['not', 'for', 'the', 'very', 'petite'], ['cagrcoal', 'shimmer', 'fun']] ...\n"
     ]
    }
   ],
   "source": [
    "def fix_typos(corpus, cutoff = 0.95, print_fixes = False, print_process = False):\n",
    "  \"\"\"\n",
    "  Fix rare typos in a corpus using difflib similarity and edit distance.\n",
    "\n",
    "  Parameters:\n",
    "      corpus: 2D list of tokens\n",
    "      min_freq: words with freq >= min_freq are considered correct dictionary\n",
    "      cutoff: similarity threshold for difflib\n",
    "      max_edit: maximum edit distance to accept as a fix\n",
    "      print_fixes: whether to print each fix\n",
    "\n",
    "  Returns:\n",
    "      fixed_corpus: corpus with typos corrected\n",
    "  \"\"\"\n",
    "  # ---- Build vocab ----\n",
    "  # Flatten corpus and compute term frequency\n",
    "  freq = calc_frequency(corpus, 'term')\n",
    "\n",
    "  # Build dictionary of frequent words\n",
    "  vocab = sorted(freq.keys(), key = lambda w: freq[w], reverse = True)\n",
    "  builtin_dict = words.words()\n",
    "  fixed_vocab = {}\n",
    "\n",
    "  potential_typos = vocab[::-1] # reverse vocab to sort freq ascendingly\n",
    "\n",
    "  # ---- Fix typos ----\n",
    "  for token in vocab:\n",
    "    if token in fixed_vocab: # token is fixed before\n",
    "      continue\n",
    "    fixed_vocab[token] = token # token has not been fixed by more frequent words => token is correct\n",
    "\n",
    "    while len(potential_typos) > 0 and freq[potential_typos[-1]] >= freq[token]: # potential typos of token should not have freq >= token\n",
    "      potential_typos.pop()\n",
    "\n",
    "    matches = difflib.get_close_matches(token, potential_typos, n=1, cutoff=cutoff) # Use difflib to find close matches\n",
    "    if matches:\n",
    "      nearest = matches[0]\n",
    "      if nearest in builtin_dict: # if nearest in dictionary, it is just a correct word which is similar to token\n",
    "        fixed_vocab[nearest] = nearest\n",
    "      else:\n",
    "        fixed_vocab[nearest] = token # Fix nearest to token\n",
    "\n",
    "  # ---- Generate new corpus based on correct tokens ----\n",
    "  fixed_corpus = []\n",
    "  for tokens in corpus:\n",
    "    fixed_tokens = []\n",
    "    for token in tokens:\n",
    "        fixed_tokens.append(fixed_vocab[token])\n",
    "    fixed_corpus.append(fixed_tokens)\n",
    "\n",
    "  # ---- Print uncorrect tokens and New corpus ----\n",
    "  if print_fixes:\n",
    "    uncorrect_vocab = sorted(fixed_vocab.items())\n",
    "    for token, fixed in uncorrect_vocab:\n",
    "      if token != fixed:\n",
    "        print(f\"Fix: {token} -> {fixed} (similar score: {difflib.SequenceMatcher(None, token, fixed).ratio()})\")\n",
    "  \n",
    "  with open('typos.txt', 'wt') as f:\n",
    "    for token, replacement in fixed_vocab.items():\n",
    "      if token != replacement:\n",
    "        f.write(f\"{token}:{replacement}\\n\")\n",
    "\n",
    "  if print_process:\n",
    "    print(\"Finish fix typos:\")\n",
    "    print(f\"+ Before: {sum([len(review_tokens) for review_tokens in corpus])} tokens: {corpus[:5]} ...\")\n",
    "    print(f\"+ Now:    {sum([len(review_tokens) for review_tokens in fixed_corpus])} tokens: {fixed_corpus[:5]} ...\")\n",
    "\n",
    "  return fixed_corpus\n",
    "\n",
    "# demo use\n",
    "print('*** This is just demo only ***')\n",
    "corpus = tokenize(df, 'Title')\n",
    "corpus_cleaned = fix_typos(corpus, print_fixes = True, print_process = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwaqjiS1MlMB"
   },
   "source": [
    "### 3.5. Full text preprocessing pipeline\n",
    "This section generates `text_preprocessing()` function to define a pipeline:\n",
    "1. Sentence segmentation and word tokenization -> use `tokenize()`.\n",
    "2. Text removal (tokens with length 1 and stop words) -> use `remove_tokens()` with \"stopword_en.txt\" file.\n",
    "3. Lemmatization -> use `lemmatize()`. This is optional, only implement if `apply_lemmatize = True`.\n",
    "4. Remove tokens with term frequency = 1 -> use `calc_frequency(type = 'term')`. This is optional, only implement if `remove_outlier = True`.\n",
    "5. Remove tokens with document frequency in top 20 highest -> use `calc_frequency(type = 'document')`. This is optional, only implement if `remove_outlier = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3067980,
     "status": "aborted",
     "timestamp": 1757247302573,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "M76uaVOBMlMB",
    "outputId": "52a170be-5a8d-4170-bde6-b4f3aa8ac3c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is just demo only ***\n",
      "\n",
      "*** Proceed df[Title And Review] ***\n",
      "Finish tokenize df[Title And Review]: 1271913 token extracted\n",
      "Finish add collocations:\n",
      "+ Before: 1271913 tokens: [['some', 'major', 'design', 'flaws', 'i', 'had', 'such', 'high', 'hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['my', 'favorite', 'buy', 'i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['flattering', 'shirt', 'this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['not', 'for', 'the', 'very', 'petite', 'i', 'love', 'tracy', 'reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "+ Now:    1270041 tokens: [['some', 'major', 'design', 'flaws', 'i', 'had-such-high-hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['my', 'favorite', 'buy', 'i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['flattering', 'shirt', 'this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['not', 'for', 'the', 'very', 'petite', 'i', 'love', 'tracy-reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "Finish fix typos:\n",
      "+ Before: 1270041 tokens: [['some', 'major', 'design', 'flaws', 'i', 'had-such-high-hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['my', 'favorite', 'buy', 'i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['flattering', 'shirt', 'this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['not', 'for', 'the', 'very', 'petite', 'i', 'love', 'tracy-reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darkler', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everythiing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirts', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "+ Now:    1270041 tokens: [['some', 'major', 'design', 'flaws', 'i', 'had-such-high-hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['my', 'favorite', 'buy', 'i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['flattering', 'shirt', 'this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['not', 'for', 'the', 'very', 'petite', 'i', 'love', 'tracy-reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darker', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everthing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirt', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "Removing stopwords and tokens with length 1\n",
      "Finish removal:\n",
      "+ Before: 1270041 tokens: [['some', 'major', 'design', 'flaws', 'i', 'had-such-high-hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', 'i', 'initially', 'ordered', 'the', 'petite', 'small', 'my', 'usual', 'size', 'but', 'i', 'found', 'this', 'to', 'be', 'outrageously', 'small', 'so', 'small', 'in', 'fact', 'that', 'i', 'could', 'not', 'zip', 'it', 'up', 'i', 'reordered', 'it', 'in', 'petite', 'medium', 'which', 'was', 'just', 'ok', 'overall', 'the', 'top', 'half', 'was', 'comfortable', 'and', 'fit', 'nicely', 'but', 'the', 'bottom', 'half', 'had', 'a', 'very', 'tight', 'under', 'layer', 'and', 'several', 'somewhat', 'cheap', 'net', 'over', 'layers', 'imo', 'a', 'major', 'design', 'flaw', 'was', 'the', 'net', 'over', 'layer', 'sewn', 'directly', 'into', 'the', 'zipper', 'it', 'c'], ['my', 'favorite', 'buy', 'i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments'], ['flattering', 'shirt', 'this', 'shirt', 'is', 'very', 'flattering', 'to', 'all', 'due', 'to', 'the', 'adjustable', 'front', 'tie', 'it', 'is', 'the', 'perfect', 'length', 'to', 'wear', 'with', 'leggings', 'and', 'it', 'is', 'sleeveless', 'so', 'it', 'pairs', 'well', 'with', 'any', 'cardigan', 'love', 'this', 'shirt'], ['not', 'for', 'the', 'very', 'petite', 'i', 'love', 'tracy-reese', 'dresses', 'but', 'this', 'one', 'is', 'not', 'for', 'the', 'very', 'petite', 'i', 'am', 'just', 'under', 'feet', 'tall', 'and', 'usually', 'wear', 'a', 'p', 'in', 'this', 'brand', 'this', 'dress', 'was', 'very', 'pretty', 'out', 'of', 'the', 'package', 'but', 'its', 'a', 'lot', 'of', 'dress', 'the', 'skirt', 'is', 'long', 'and', 'very', 'full', 'so', 'it', 'overwhelmed', 'my', 'small', 'frame', 'not', 'a', 'stranger', 'to', 'alterations', 'shortening', 'and', 'narrowing', 'the', 'skirt', 'would', 'take', 'away', 'from', 'the', 'embellishment', 'of', 'the', 'garment', 'i', 'love', 'the', 'color', 'and', 'the', 'idea', 'of', 'the', 'style', 'but', 'it', 'just', 'did', 'not', 'work', 'on', 'me', 'i', 'returned', 'this', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'i', 'aded', 'this', 'in', 'my', 'basket', 'at', 'hte', 'last', 'mintue', 'to', 'see', 'what', 'it', 'would', 'look', 'like', 'in', 'person', 'store', 'pick', 'up', 'i', 'went', 'with', 'teh', 'darker', 'color', 'only', 'because', 'i', 'am', 'so', 'pale', 'hte', 'color', 'is', 'really', 'gorgeous', 'and', 'turns', 'out', 'it', 'mathced', 'everthing', 'i', 'was', 'trying', 'on', 'with', 'it', 'prefectly', 'it', 'is', 'a', 'little', 'baggy', 'on', 'me', 'and', 'hte', 'xs', 'is', 'hte', 'msallet', 'size', 'bummer', 'no', 'petite', 'i', 'decided', 'to', 'jkeep', 'it', 'though', 'because', 'as', 'i', 'said', 'it', 'matvehd', 'everything', 'my', 'ejans', 'pants', 'and', 'the', 'skirt', 'i', 'waas', 'trying', 'on', 'of', 'which', 'i', 'kept', 'all', 'oops']] ...\n",
      "+ Now:    494535 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'dress', 'wanted', 'work', 'initially', 'ordered', 'petite', 'small', 'usual', 'size', 'found', 'outrageously', 'small', 'small', 'fact', 'zip', 'reordered', 'petite', 'medium', 'top', 'half', 'comfortable', 'fit', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'wear', 'great', 'compliments'], ['flattering', 'shirt', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', 'perfect', 'length', 'wear', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'love', 'shirt'], ['petite', 'love', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'wear', 'brand', 'dress', 'pretty', 'package', 'lot', 'dress', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', 'stranger', 'alterations', 'shortening', 'narrowing', 'skirt', 'embellishment', 'garment', 'love', 'color', 'idea', 'style', 'work', 'returned', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'aded', 'basket', 'hte', 'mintue', 'person', 'store', 'pick', 'teh', 'darker', 'color', 'pale', 'hte', 'color', 'gorgeous', 'turns', 'mathced', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'msallet', 'size', 'bummer', 'petite', 'decided', 'jkeep', 'matvehd', 'ejans', 'pants', 'skirt', 'waas', 'oops']] ...\n",
      "Removing tokens with term frequency = 1\n",
      "Finish removal:\n",
      "+ Before: 494535 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'dress', 'wanted', 'work', 'initially', 'ordered', 'petite', 'small', 'usual', 'size', 'found', 'outrageously', 'small', 'small', 'fact', 'zip', 'reordered', 'petite', 'medium', 'top', 'half', 'comfortable', 'fit', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'wear', 'great', 'compliments'], ['flattering', 'shirt', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', 'perfect', 'length', 'wear', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'love', 'shirt'], ['petite', 'love', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'wear', 'brand', 'dress', 'pretty', 'package', 'lot', 'dress', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', 'stranger', 'alterations', 'shortening', 'narrowing', 'skirt', 'embellishment', 'garment', 'love', 'color', 'idea', 'style', 'work', 'returned', 'dress'], ['cagrcoal', 'shimmer', 'fun', 'aded', 'basket', 'hte', 'mintue', 'person', 'store', 'pick', 'teh', 'darker', 'color', 'pale', 'hte', 'color', 'gorgeous', 'turns', 'mathced', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'msallet', 'size', 'bummer', 'petite', 'decided', 'jkeep', 'matvehd', 'ejans', 'pants', 'skirt', 'waas', 'oops']] ...\n",
      "+ Now:    488286 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'dress', 'wanted', 'work', 'initially', 'ordered', 'petite', 'small', 'usual', 'size', 'found', 'outrageously', 'small', 'small', 'fact', 'zip', 'reordered', 'petite', 'medium', 'top', 'half', 'comfortable', 'fit', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'wear', 'great', 'compliments'], ['flattering', 'shirt', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', 'perfect', 'length', 'wear', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'love', 'shirt'], ['petite', 'love', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'wear', 'brand', 'dress', 'pretty', 'package', 'lot', 'dress', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', 'stranger', 'alterations', 'shortening', 'skirt', 'embellishment', 'garment', 'love', 'color', 'idea', 'style', 'work', 'returned', 'dress'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'color', 'pale', 'hte', 'color', 'gorgeous', 'turns', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'size', 'bummer', 'petite', 'decided', 'ejans', 'pants', 'skirt', 'oops']] ...\n",
      "Removing tokens with document frequency in top 20\n",
      "Finish removal:\n",
      "+ Before: 488286 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'dress', 'wanted', 'work', 'initially', 'ordered', 'petite', 'small', 'usual', 'size', 'found', 'outrageously', 'small', 'small', 'fact', 'zip', 'reordered', 'petite', 'medium', 'top', 'half', 'comfortable', 'fit', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'wear', 'great', 'compliments'], ['flattering', 'shirt', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', 'perfect', 'length', 'wear', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'love', 'shirt'], ['petite', 'love', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'wear', 'brand', 'dress', 'pretty', 'package', 'lot', 'dress', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', 'stranger', 'alterations', 'shortening', 'skirt', 'embellishment', 'garment', 'love', 'color', 'idea', 'style', 'work', 'returned', 'dress'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'color', 'pale', 'hte', 'color', 'gorgeous', 'turns', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'size', 'bummer', 'petite', 'decided', 'ejans', 'pants', 'skirt', 'oops']] ...\n",
      "+ Now:    383639 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'wanted', 'work', 'initially', 'petite', 'usual', 'found', 'outrageously', 'fact', 'zip', 'reordered', 'petite', 'medium', 'half', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliments'], ['shirt', 'shirt', 'due', 'adjustable', 'front', 'tie', 'length', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'shirt'], ['petite', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'brand', 'pretty', 'package', 'lot', 'skirt', 'long', 'full', 'overwhelmed', 'frame', 'stranger', 'alterations', 'shortening', 'skirt', 'embellishment', 'garment', 'idea', 'style', 'work', 'returned'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'pale', 'hte', 'gorgeous', 'turns', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'bummer', 'petite', 'decided', 'ejans', 'pants', 'skirt', 'oops']] ...\n",
      "Finish lemmatize:\n",
      "+ Before: 383639 tokens: [['major', 'design', 'flaws', 'had-such-high-hopes', 'wanted', 'work', 'initially', 'petite', 'usual', 'found', 'outrageously', 'fact', 'zip', 'reordered', 'petite', 'medium', 'half', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layers', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliments'], ['shirt', 'shirt', 'due', 'adjustable', 'front', 'tie', 'length', 'leggings', 'sleeveless', 'pairs', 'cardigan', 'shirt'], ['petite', 'tracy-reese', 'dresses', 'petite', 'feet', 'tall', 'brand', 'pretty', 'package', 'lot', 'skirt', 'long', 'full', 'overwhelmed', 'frame', 'stranger', 'alterations', 'shortening', 'skirt', 'embellishment', 'garment', 'idea', 'style', 'work', 'returned'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'pale', 'hte', 'gorgeous', 'turns', 'everthing', 'prefectly', 'baggy', 'hte', 'xs', 'hte', 'bummer', 'petite', 'decided', 'ejans', 'pants', 'skirt', 'oops']] ...\n",
      "+ Now:    383639 tokens: [['major', 'design', 'flaw', 'had-such-high-hopes', 'want', 'work', 'initially', 'petite', 'usual', 'find', 'outrageously', 'fact', 'zip', 'reorder', 'petite', 'medium', 'half', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layer', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliment'], ['shirt', 'shirt', 'due', 'adjustable', 'front', 'tie', 'length', 'legging', 'sleeveless', 'pair', 'cardigan', 'shirt'], ['petite', 'tracy-reese', 'dress', 'petite', 'foot', 'tall', 'brand', 'pretty', 'package', 'lot', 'skirt', 'long', 'full', 'overwhelm', 'frame', 'stranger', 'alteration', 'shorten', 'skirt', 'embellishment', 'garment', 'idea', 'style', 'work', 'return'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'pale', 'hte', 'gorgeous', 'turn', 'everthing', 'prefectly', 'baggy', 'hte', 'x', 'hte', 'bummer', 'petite', 'decide', 'ejans', 'pant', 'skirt', 'oops']] ...\n",
      "Remove stopwords again after lemmatization\n",
      "Finish removal:\n",
      "+ Before: 383639 tokens: [['major', 'design', 'flaw', 'had-such-high-hopes', 'want', 'work', 'initially', 'petite', 'usual', 'find', 'outrageously', 'fact', 'zip', 'reorder', 'petite', 'medium', 'half', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layer', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliment'], ['shirt', 'shirt', 'due', 'adjustable', 'front', 'tie', 'length', 'legging', 'sleeveless', 'pair', 'cardigan', 'shirt'], ['petite', 'tracy-reese', 'dress', 'petite', 'foot', 'tall', 'brand', 'pretty', 'package', 'lot', 'skirt', 'long', 'full', 'overwhelm', 'frame', 'stranger', 'alteration', 'shorten', 'skirt', 'embellishment', 'garment', 'idea', 'style', 'work', 'return'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'pale', 'hte', 'gorgeous', 'turn', 'everthing', 'prefectly', 'baggy', 'hte', 'x', 'hte', 'bummer', 'petite', 'decide', 'ejans', 'pant', 'skirt', 'oops']] ...\n",
      "+ Now:    376639 tokens: [['major', 'design', 'flaw', 'had-such-high-hopes', 'work', 'initially', 'petite', 'usual', 'find', 'outrageously', 'fact', 'zip', 'reorder', 'petite', 'medium', 'half', 'nicely', 'bottom', 'half', 'tight', 'layer', 'cheap', 'net', 'layer', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper'], ['favorite', 'buy', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliment'], ['shirt', 'shirt', 'due', 'adjustable', 'front', 'tie', 'length', 'legging', 'sleeveless', 'pair', 'cardigan', 'shirt'], ['petite', 'tracy-reese', 'dress', 'petite', 'foot', 'tall', 'brand', 'pretty', 'package', 'lot', 'skirt', 'long', 'full', 'overwhelm', 'frame', 'stranger', 'alteration', 'shorten', 'skirt', 'embellishment', 'garment', 'idea', 'style', 'work', 'return'], ['shimmer', 'fun', 'basket', 'hte', 'person', 'store', 'pick', 'teh', 'darker', 'pale', 'hte', 'gorgeous', 'turn', 'everthing', 'prefectly', 'baggy', 'hte', 'hte', 'bummer', 'petite', 'decide', 'ejans', 'pant', 'skirt', 'oops']] ...\n",
      "Finish proceed df[Title And Review]: remain 6658 unique tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Clothes Title</th>\n",
       "      <th>Clothes Description</th>\n",
       "      <th>Title And Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Elegant A-Line Dress</td>\n",
       "      <td>A classic A-line dress that flows gracefully, ...</td>\n",
       "      <td>major design flaw had-such-high-hopes work ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>Petite High-Waisted Trousers</td>\n",
       "      <td>Chic, high-waisted trousers designed to elonga...</td>\n",
       "      <td>favorite buy jumpsuit fun flirty fabulous time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>Silk Button-Up Blouse</td>\n",
       "      <td>A luxurious silk blouse with a timeless button...</td>\n",
       "      <td>shirt shirt due adjustable front tie length le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Elegant A-Line Dress</td>\n",
       "      <td>A classic A-line dress that flows gracefully, ...</td>\n",
       "      <td>petite tracy-reese dress petite foot tall bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Petite Cable Knit Sweater</td>\n",
       "      <td>A cozy cable knit sweater tailored specificall...</td>\n",
       "      <td>shimmer fun basket hte person store pick teh d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Petite Floral Midi Dress</td>\n",
       "      <td>A beautiful floral midi dress designed for pet...</td>\n",
       "      <td>occasion happy snag price easy slip cut combo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Petite Cable Knit Sweater</td>\n",
       "      <td>A cozy cable knit sweater tailored specificall...</td>\n",
       "      <td>make cotton remind maternity clothes stretchy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Petite Floral Midi Dress</td>\n",
       "      <td>A beautiful floral midi dress designed for pet...</td>\n",
       "      <td>work glad store order online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Elegant A-Line Dress</td>\n",
       "      <td>A classic A-line dress that flows gracefully, ...</td>\n",
       "      <td>summer party buy wed summer medium waist perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19661</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Petite Floral Midi Dress</td>\n",
       "      <td>A beautiful floral midi dress designed for pet...</td>\n",
       "      <td>make lovely feminine perfectly easy comfy high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19662 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19657         1104   34                     Great dress for many occasions   \n",
       "19658          862   48                         Wish it was made of cotton   \n",
       "19659         1104   31                              Cute, but see through   \n",
       "19660         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19661         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19657  I was very happy to snag this dress at such a ...       5   \n",
       "19658  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19659  This fit well, but the top was very see throug...       3   \n",
       "19660  I bought this dress for a wedding i have this ...       3   \n",
       "19661  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19657                1                        0  General Petite   \n",
       "19658                1                        0  General Petite   \n",
       "19659                0                        1  General Petite   \n",
       "19660                1                        2         General   \n",
       "19661                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name                 Clothes Title  \\\n",
       "0             Dresses    Dresses          Elegant A-Line Dress   \n",
       "1             Bottoms      Pants  Petite High-Waisted Trousers   \n",
       "2                Tops    Blouses         Silk Button-Up Blouse   \n",
       "3             Dresses    Dresses          Elegant A-Line Dress   \n",
       "4                Tops      Knits     Petite Cable Knit Sweater   \n",
       "...               ...        ...                           ...   \n",
       "19657         Dresses    Dresses      Petite Floral Midi Dress   \n",
       "19658            Tops      Knits     Petite Cable Knit Sweater   \n",
       "19659         Dresses    Dresses      Petite Floral Midi Dress   \n",
       "19660         Dresses    Dresses          Elegant A-Line Dress   \n",
       "19661         Dresses    Dresses      Petite Floral Midi Dress   \n",
       "\n",
       "                                     Clothes Description  \\\n",
       "0      A classic A-line dress that flows gracefully, ...   \n",
       "1      Chic, high-waisted trousers designed to elonga...   \n",
       "2      A luxurious silk blouse with a timeless button...   \n",
       "3      A classic A-line dress that flows gracefully, ...   \n",
       "4      A cozy cable knit sweater tailored specificall...   \n",
       "...                                                  ...   \n",
       "19657  A beautiful floral midi dress designed for pet...   \n",
       "19658  A cozy cable knit sweater tailored specificall...   \n",
       "19659  A beautiful floral midi dress designed for pet...   \n",
       "19660  A classic A-line dress that flows gracefully, ...   \n",
       "19661  A beautiful floral midi dress designed for pet...   \n",
       "\n",
       "                                        Title And Review  \n",
       "0      major design flaw had-such-high-hopes work ini...  \n",
       "1      favorite buy jumpsuit fun flirty fabulous time...  \n",
       "2      shirt shirt due adjustable front tie length le...  \n",
       "3      petite tracy-reese dress petite foot tall bran...  \n",
       "4      shimmer fun basket hte person store pick teh d...  \n",
       "...                                                  ...  \n",
       "19657      occasion happy snag price easy slip cut combo  \n",
       "19658  make cotton remind maternity clothes stretchy ...  \n",
       "19659                       work glad store order online  \n",
       "19660  summer party buy wed summer medium waist perfe...  \n",
       "19661  make lovely feminine perfectly easy comfy high...  \n",
       "\n",
       "[19662 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to execute all text preprocessing pipeline\n",
    "def text_preprocessing(original_df, attribute, remove_outlier = False, ngram_range = [2, 3, 4], print_process = False):\n",
    "  '''\n",
    "    Implement full process of text preprocessing with the steps.\n",
    "\n",
    "    Agrs:\n",
    "      df (pd.DataFrame): Dataset\n",
    "      attribute (str): the text attribute of df for preprocessing\n",
    "      detect_outliers (bool): Implement step 4 or not\n",
    "\n",
    "    Returns:\n",
    "      df (pd.DataFrame): Dataset after preprocessing\n",
    "      vocab (dict): The vocabulary of all unique tokens extracted, with their id (in alphabetical order)\n",
    "  '''\n",
    "  df = original_df.copy()\n",
    "  if print_process:\n",
    "    print(f\"\\n*** Proceed df[{attribute}] ***\")\n",
    "\n",
    "  # ---- Sentence segmentation -> word tokenization ----\n",
    "  corpus = tokenize(df, attribute, print_process = print_process)\n",
    "  removed_tokens = set()\n",
    "\n",
    "  # ---- Find and add collocations ----\n",
    "  collocations = find_collocations(corpus, ngram_range = ngram_range)\n",
    "  corpus = add_collocations(corpus, collocations, print_process = print_process)\n",
    "  with open('collocations.txt', 'wt') as f:\n",
    "    for collocation in collocations:\n",
    "      f.write(f\"{collocation}\\n\")\n",
    "\n",
    "  # ---- Fix typos ----\n",
    "  corpus = fix_typos(corpus, cutoff = 0.9, print_process = print_process)\n",
    "\n",
    "  # ---- Text removal (single char + stop words) ----\n",
    "  if print_process:\n",
    "    print('Removing stopwords and tokens with length 1')\n",
    "\n",
    "  with open(\"stopwords_en.txt\", \"r\", encoding=\"utf-8\") as f: # Download stopwords_en.txt\n",
    "    stop_words = set(w.strip().lower() for w in f if w.strip())\n",
    "  corpus = remove_tokens(corpus, stop_words, remove_single_char = True, print_process = print_process) # Text removal: stopwords + tokens with length = 1\n",
    "  removed_tokens.update(stop_words)\n",
    "\n",
    "  # ---- Remove rare tokens + Remove frequent tokens ----\n",
    "  if remove_outlier: # Only run if the user allow (when detect_outliers = True)\n",
    "\n",
    "    # Remove tokens with term frequency = 1\n",
    "    term_freq = calc_frequency(corpus, type=\"term\") # calculate term frequency\n",
    "    rare_tokens = set(t for t, f in term_freq.items() if f == 1) # filter tokens with term frequency = 1\n",
    "\n",
    "    if print_process:\n",
    "      print('Removing tokens with term frequency = 1')\n",
    "    corpus = remove_tokens(corpus, rare_tokens, print_process = print_process) # remove rare tokens\n",
    "    removed_tokens.update(rare_tokens)\n",
    "\n",
    "    # Remove tokens with document frequency in top 20\n",
    "    doc_freq = calc_frequency(corpus, type=\"document\") # calculate document frequency\n",
    "    top_tokens = set(sorted(doc_freq, key=lambda x: doc_freq[x], reverse=True)[:20]) # filter top 20 most frequent tokens (document frequency)\n",
    "\n",
    "    if print_process:\n",
    "      print('Removing tokens with document frequency in top 20')\n",
    "    corpus = remove_tokens(corpus, top_tokens, print_process = print_process) # remove frequent tokens\n",
    "    removed_tokens.update(top_tokens)\n",
    "\n",
    "  # ---- Lemmatization ----\n",
    "  corpus = lemmatize(corpus, print_process = print_process)\n",
    "\n",
    "  if print_process:\n",
    "    print('Remove stopwords again after lemmatization')\n",
    "  corpus = remove_tokens(corpus, stop_words, remove_single_char = True, print_process = print_process)\n",
    "\n",
    "  # ----- Output -----\n",
    "  # Apply final cleaned text to the dataset\n",
    "  processed_texts = [' '.join([t for t in doc]) for doc in corpus] # each row in df[attribute] will be a string (combination of tokens separated by \" \")\n",
    "  df[attribute] = processed_texts\n",
    "\n",
    "  # Build vocab dict (alphabetical order)\n",
    "  unique_tokens = sorted({t for doc in corpus for t in doc})\n",
    "  vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "  # Save removed tokens\n",
    "  with open('removed_tokens.txt', 'wt') as f:\n",
    "    for token in removed_tokens:\n",
    "      f.write(f\"{token}\\n\")\n",
    "\n",
    "  print(f\"Finish proceed df[{attribute}]: remain {len(vocab)} unique tokens\")\n",
    "\n",
    "  return df, vocab\n",
    "\n",
    "# demo use\n",
    "print('*** This is just demo only ***')\n",
    "df['Title And Review'] = (df['Title'] + ' ' + df['Review Text']).str.strip()\n",
    "processed_df, vocab_both = text_preprocessing(df, 'Title And Review', remove_outlier = True, print_process = True)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsME2_PsMlMD"
   },
   "source": [
    "## 4. Saving required outputs\n",
    "Save the requested information as per specification.\n",
    "+ \"vocab.txt\": unigram, cleaned vocabulary of *Review Text* attribute of \"assignment3.csv\" dataset (stored in `df`).\n",
    "+ \"processed.csv\": The dataset, of which attribute *Review Text* is implemented through text processing, and only the words including in the vobabulary (stored in \"vocab.txt\") are stored, separated by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 2908121,
     "status": "aborted",
     "timestamp": 1757247302575,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "A8LFrCEliQba",
    "outputId": "9d196bb5-159b-4a20-9f53-860f9d34e845"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Title And Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Some major design flaws I had such high hopes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>My favorite buy! I love, love, love this jumps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>Flattering shirt This shirt is very flattering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Not for the very petite I love tracy reese dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Cagrcoal shimmer fun I aded this in my basket ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0         General         Dresses    Dresses   \n",
       "1                        0  General Petite         Bottoms      Pants   \n",
       "2                        6         General            Tops    Blouses   \n",
       "3                        4         General         Dresses    Dresses   \n",
       "4                        1  General Petite            Tops      Knits   \n",
       "\n",
       "                                    Title And Review  \n",
       "0  Some major design flaws I had such high hopes ...  \n",
       "1  My favorite buy! I love, love, love this jumps...  \n",
       "2  Flattering shirt This shirt is very flattering...  \n",
       "3  Not for the very petite I love tracy reese dre...  \n",
       "4  Cagrcoal shimmer fun I aded this in my basket ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add \"Title And Review\" column as a combination of \"Title\" and \"Review Text\"\n",
    "df['Title And Review'] = (df['Title'] + ' ' + df['Review Text']).str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PvrRPEBk0G6"
   },
   "outputs": [],
   "source": [
    "# Function to save df and vocab after text preprocessing pipeline\n",
    "def save_processed(df, vocab, df_filename, vocab_filename):\n",
    "  df.to_csv(df_filename)\n",
    "  with open(vocab_filename, 'wt') as f:\n",
    "    for token, i in vocab.items():\n",
    "      f.write(f'{token}:{i}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2907927,
     "status": "aborted",
     "timestamp": 1757247302577,
     "user": {
      "displayName": "Tô Minh Tuấn",
      "userId": "13812075662849477544"
     },
     "user_tz": -420
    },
    "id": "ae-ULgDciSwe",
    "outputId": "08eab90a-f9b1-4934-8e44-e799c00c7f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish proceed df[Review Text]: remain 6347 unique tokens\n",
      "Finish proceed df[Title]: remain 1464 unique tokens\n",
      "Finish proceed df[Title And Review]: remain 6641 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# save result when implement whole pipeline using n-grams collocation detection with n = [2, 3]\n",
    "\n",
    "# processed_df = df.copy()\n",
    "# processed_df, vocab_text = text_preprocessing(processed_df, 'Review Text', remove_outlier = True, ngram_range = [2, 3])\n",
    "# processed_df, vocab_title = text_preprocessing(processed_df, 'Title', remove_outlier = True, ngram_range = [2, 3])\n",
    "# processed_df, vocab_both = text_preprocessing(processed_df, 'Title And Review', remove_outlier = True, ngram_range = [2, 3])\n",
    "\n",
    "# save_processed(processed_df, vocab_text, 'processed_23.csv', 'vocab_text_23.txt')\n",
    "# save_processed(processed_df, vocab_title, 'processed_23.csv', 'vocab_title_23.txt')\n",
    "# save_processed(processed_df, vocab_both, 'processed_23.csv', 'vocab_both_23.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "error",
     "timestamp": 1757303598880,
     "user": {
      "displayName": "Dylan Tran",
      "userId": "08208623625250116136"
     },
     "user_tz": -420
    },
    "id": "wrDWaes6qFg3",
    "outputId": "2c317bd8-1ef1-4f0b-e7d3-3040aae2d4c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Title And Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>major design flaw</td>\n",
       "      <td>high hope work initially petite usual find out...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>major design flaws high hope work initially pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>favorite buy</td>\n",
       "      <td>jumpsuit fun flirty fabulous time compliment</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>favorite buy jumpsuit fun flirty fabulous time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>shirt</td>\n",
       "      <td>shirt due adjustable front tie length legging ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>shirt shirt due adjustable front tie length le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>petite</td>\n",
       "      <td>tracy-reese dress petite foot tall brand prett...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>petite tracy-reese dress petite foot tall bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>shimmer fun</td>\n",
       "      <td>basket hte person store pick teh darker pale h...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>shimmer fun basket hte person store pick teh d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age              Title  \\\n",
       "0         1077   60  major design flaw   \n",
       "1         1049   50       favorite buy   \n",
       "2          847   47              shirt   \n",
       "3         1080   49             petite   \n",
       "4          858   39        shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  high hope work initially petite usual find out...       3                0   \n",
       "1       jumpsuit fun flirty fabulous time compliment       5                1   \n",
       "2  shirt due adjustable front tie length legging ...       5                1   \n",
       "3  tracy-reese dress petite foot tall brand prett...       2                0   \n",
       "4  basket hte person store pick teh darker pale h...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0         General         Dresses    Dresses   \n",
       "1                        0  General Petite         Bottoms      Pants   \n",
       "2                        6         General            Tops    Blouses   \n",
       "3                        4         General         Dresses    Dresses   \n",
       "4                        1  General Petite            Tops      Knits   \n",
       "\n",
       "                                    Title And Review  \n",
       "0  major design flaws high hope work initially pe...  \n",
       "1  favorite buy jumpsuit fun flirty fabulous time...  \n",
       "2  shirt shirt due adjustable front tie length le...  \n",
       "3  petite tracy-reese dress petite foot tall bran...  \n",
       "4  shimmer fun basket hte person store pick teh d...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the dataset again to check\n",
    "# processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-GXK2ZclwC4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish proceed df[Review Text]: remain 6362 unique tokens\n",
      "Finish proceed df[Title]: remain 1477 unique tokens\n",
      "Finish proceed df[Title And Review]: remain 6658 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# save result when implement whole pipeline using n-grams collocation detection with n = [2, 3, 4]\n",
    "\n",
    "processed_df = df.copy()\n",
    "processed_df, vocab_text = text_preprocessing(processed_df, 'Review Text', remove_outlier = True, ngram_range = [2, 3, 4])\n",
    "processed_df, vocab_title = text_preprocessing(processed_df, 'Title', remove_outlier = True, ngram_range = [2, 3, 4])\n",
    "processed_df, vocab_both = text_preprocessing(processed_df, 'Title And Review', remove_outlier = True, ngram_range = [2, 3, 4])\n",
    "\n",
    "# support model_experiment.ipynb and task2_3.ipynb\n",
    "# save_processed(processed_df, vocab_text, 'processed_234.csv', 'vocab_text_234.txt')\n",
    "# save_processed(processed_df, vocab_title, 'processed_234.csv', 'vocab_title_234.txt')\n",
    "# save_processed(processed_df, vocab_both, 'processed_234.csv', 'vocab_both_234.txt')\n",
    "\n",
    "# save result for Task 1\n",
    "# save_processed(processed_df, vocab_text, 'processed.csv', 'vocab.txt') # vocab.txt store vocab of Review Text -- as required by Task 1 requirements\n",
    "# save_processed(processed_df, vocab_text, 'processed.csv', 'vocab_text.txt')\n",
    "# save_processed(processed_df, vocab_title, 'processed.csv', 'vocab_title.txt')\n",
    "# save_processed(processed_df, vocab_both, 'processed.csv', 'vocab_both.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj0Rm5c3pwBg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Title And Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>major design flaw</td>\n",
       "      <td>had-such-high-hopes work initially petite usua...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>major design flaw had-such-high-hopes work ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>favorite buy</td>\n",
       "      <td>jumpsuit fun flirty fabulous time compliment</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>favorite buy jumpsuit fun flirty fabulous time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>shirt</td>\n",
       "      <td>shirt due adjustable front tie length legging ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>shirt shirt due adjustable front tie length le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>petite</td>\n",
       "      <td>tracy-reese dress petite foot tall brand prett...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>petite tracy-reese dress petite foot tall bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>shimmer fun</td>\n",
       "      <td>basket hte person store pick teh darker pale h...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>shimmer fun basket hte person store pick teh d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age              Title  \\\n",
       "0         1077   60  major design flaw   \n",
       "1         1049   50       favorite buy   \n",
       "2          847   47              shirt   \n",
       "3         1080   49             petite   \n",
       "4          858   39        shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  had-such-high-hopes work initially petite usua...       3                0   \n",
       "1       jumpsuit fun flirty fabulous time compliment       5                1   \n",
       "2  shirt due adjustable front tie length legging ...       5                1   \n",
       "3  tracy-reese dress petite foot tall brand prett...       2                0   \n",
       "4  basket hte person store pick teh darker pale h...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0         General         Dresses    Dresses   \n",
       "1                        0  General Petite         Bottoms      Pants   \n",
       "2                        6         General            Tops    Blouses   \n",
       "3                        4         General         Dresses    Dresses   \n",
       "4                        1  General Petite            Tops      Knits   \n",
       "\n",
       "                                    Title And Review  \n",
       "0  major design flaw had-such-high-hopes work ini...  \n",
       "1  favorite buy jumpsuit fun flirty fabulous time...  \n",
       "2  shirt shirt due adjustable front tie length le...  \n",
       "3  petite tracy-reese dress petite foot tall bran...  \n",
       "4  shimmer fun basket hte person store pick teh d...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the dataset again to check\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to read vocabulary file to a Python dict()\n",
    "def read_vocab(filename):\n",
    "  vocab = {}\n",
    "  with open(filename, 'r') as f:\n",
    "    vocab = {line.split(':')[0]: int(line.split(':')[1]) for line in f} # Convert index to integer\n",
    "  return vocab\n",
    "\n",
    "# Function to calculate weighted vectors (document representation) based on an embedding model loaded in advance\n",
    "def calc_weighted_vectors(df, attribute, vocab_dict, model, tfidf_vectorizer = None):\n",
    "  '''\n",
    "  Calculates TF-IDF weighted document vectors.\n",
    "\n",
    "  Args:\n",
    "    df: The DataFrame containing the text data.\n",
    "    attribute: The column name in the DataFrame with the text.\n",
    "    vocab_dict: A dictionary mapping vocabulary tokens to their unique IDs.\n",
    "    model: The pre-trained word embedding model (e.g., Word2Vec, FastText).\n",
    "\n",
    "  Returns:\n",
    "    numpy.ndarray: A 2D array where each row is the weighted vector for a document.\n",
    "  '''\n",
    "  # Use TfidfVectorizer with the predefined vocabulary to get TF-IDF scores\n",
    "  if tfidf_vectorizer is None:\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', vocabulary = vocab_dict, lowercase = True)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[attribute].fillna('')) # Fill NaN values with empty strings before vectorization\n",
    "  else:\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(df[attribute].fillna(''))\n",
    "\n",
    "  # Precompute embedding matrix aligned with vocab_dict\n",
    "  embedding_matrix = np.zeros((len(vocab_dict), model.wv.vector_size))\n",
    "  for token, idx in vocab_dict.items():\n",
    "    if token in model.wv.key_to_index:  # Check if token exists in pretrained model\n",
    "      embedding_matrix[idx] = model.wv[token]\n",
    "    # else remains zero vector\n",
    "\n",
    "  # Compute Weighted Review Vectors (TF-IDF weighted mean)\n",
    "  weighted_vectors = []\n",
    "  for doc_idx in range(tfidf_matrix.shape[0]):\n",
    "    row = tfidf_matrix.getrow(doc_idx)\n",
    "    indices = row.indices # only get element that is not = 0\n",
    "    weights = row.data\n",
    "\n",
    "    if len(indices) == 0:\n",
    "      weighted_vectors.append(np.zeros(model.wv.vector_size))\n",
    "      continue\n",
    "\n",
    "    # Get the corresponding word vectors from the precomputed embedding matrix\n",
    "    word_vecs = embedding_matrix[indices]\n",
    "\n",
    "    # Perform a dot product to get the weighted sum\n",
    "    weighted_sum = np.dot(weights, word_vecs)\n",
    "    weighted_avg = weighted_sum / weights.sum()\n",
    "    weighted_vectors.append(weighted_avg)\n",
    "\n",
    "  return np.vstack(weighted_vectors)  # shape: (n_docs, vector_size)\n",
    "\n",
    "# ---- Define model to predict (vote of 3 models) ----\n",
    "\n",
    "# df = pd.read_csv('processed.csv')\n",
    "vocab_both = read_vocab('vocab_both.txt')\n",
    "fasttext_both_model = FastText.load('fasttext_model.model')\n",
    "\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "vote_clf = joblib.load('voting_classifier.pkl')\n",
    "\n",
    "# demo use of pipeline to fit and predict\n",
    "data = pd.DataFrame({\n",
    "  'New Review': ['I love this skirt', 'I hate this', 'had such high hopes', ' I dont like it', 'Yuck', 'Would love to recommend it']\n",
    "})\n",
    "wv = calc_weighted_vectors(data, 'New Review', vocab_both, fasttext_both_model, tfidf_vectorizer)\n",
    "print(vote_clf.predict(wv))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gensim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
